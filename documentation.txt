1) Created the environnment
2) Switched from Google Colab to local environment jupyter notebook.
3) Switched form Jupyter notebook to PyCharm.
4) Tried fixing the orientation and the size of the simulation's visuals
5) Testing the Car racing game's environment but initially giving random inputs to the car (e.g random level of acceleration, randomly applying brakes, random steering direction, etc.)
6) Facing the error stating: 'AttributeError: module 'numpy' has no attribute 'bool8'. Did you mean: 'bool'?
'
7) Fixed the issue by making changes in the internal numpy library's file itself: replaced np.bool8 with np.bool_
8) The simulation works but the vehicle is not moving right now.
9) I have set the random values for 'throttle' and 'steering'. It is running
10) Creating a logging system that creates a new log file every new day and I can revisit my past logs as well
11) Studying the 'state' return type of my env.action(). Will possibly preprocess this and use it as input for my model
12) Thinking of DQN + PPO approach to build and train a strong model
13) Studying the 'state' N-dimensional vector. This will prove out to be helpful
14) Starting to think from first principles and am this time thinking of CNN + PPO.
15) Since I have no labelled data, I am trying to formulate an approach to build and train an agent
16) Studied stable_baseline3 library to implement PPO reinforcement learning algorithm
17) Implementing the PPO(cnn_policy) and setting things up
18) Facing multiple issues in car_dynamics.py. Fixing those internal errors manually
19) So far worked on fixing the data type issues and the incompatability with box-2d
20) Installing box2d-py, for which my linux machine required 'swig'
21) A reminder that for this project, one also needs to directly use my 'venv' because there are certain internal changes which I have made in the modules of the OpenAI's gym library and NumPy's library to handle certain compatability and type issues
22) Converted np.float32 and np.float64 types to basic Python float in point coordinates before calling pygame.draw.polygon() to ensure compatibility and avoid potential issues with pygame rendering. Improved code stability by handling different numeric types in the points list.
23) It worked!!! I made multiple changes to car_dynamics.py, car_racing.py, polygon functions and point tuples
24) Finally, the model is training and I can even view the simulation
25) There are actually a new set of issues:
 a) total_timesteps (the number of actions taken by the agent) is not making any difference
 b) The simulation starts with the car moving and basically the model running it for around 10 seconds, after which the simulation automatically resets. This happens 3 times and at the end of the third reset and re-run of the simulation, I get these:
"
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -54.6    |
| time/              |          |
|    fps             | 42       |
|    iterations      | 1        |
|    time_elapsed    | 48       |
|    total_timesteps | 2048     |
---------------------------------
"

Right after I get these my pygame/simulation gui freezes and I get this pop-up that pygame is not responding

Trying to do something about these
26) Fixed issue a) by studying the documentation and finding out a new parameter n_steps in PPO's Cnnpolicy.
The reason why total_timesteps was not making any difference was because technically n_steps of PPO's Cnnpolicy takes  has more priority in deciding
the number of steps the agent will take. Hence, we need to set both of them actually.

27) Fixed the second issue by further simplification and further cross examination of the working of each 'episode' of an agent
28)
'
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -57.5    |
| time/              |          |
|    fps             | 47       |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 1000     |
---------------------------------
2025-02-07 09:44:22 - âœ… Training Complete. Saving model...
2025-02-07 09:44:22 - ðŸ’¾ Model saved as 'ppo_CarRacing-v2_cnn'.
'
My next target is to increase the number of iterations and slowly keep improving over the system until the Episode Reward Mean doesn't become in positive and then greater than 100 or so
29) It still freezes. Trying to change render_mode in gym.make() method
30) - Replacing `gym` with `gymnasium`, removing `render_mode='human'`, increasing `batch_size` to 64 and `n_steps` to 2048, reducing logging verbosity, optimizing logs to prevent PyCharm freeze, and ensuring model checkpoints are saving correctlyâ€”improving stability, performance, and training efficiency while preventing crashes.
31) It was due to me trying to visualize the training parallely using PyCharm. Hence, I closed that.