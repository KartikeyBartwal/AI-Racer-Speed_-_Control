1) Created the environnment
2) Switched from Google Colab to local environment jupyter notebook.
3) Switched form Jupyter notebook to PyCharm.
4) Tried fixing the orientation and the size of the simulation's visuals
5) Testing the Car racing game's environment but initially giving random inputs to the car (e.g random level of acceleration, randomly applying brakes, random steering direction, etc.)
6) Facing the error stating: 'AttributeError: module 'numpy' has no attribute 'bool8'. Did you mean: 'bool'?
'
7) Fixed the issue by making changes in the internal numpy library's file itself: replaced np.bool8 with np.bool_
8) The simulation works but the vehicle is not moving right now.
9) I have set the random values for 'throttle' and 'steering'. It is running
10) Creating a logging system that creates a new log file every new day and I can revisit my past logs as well
11) Studying the 'state' return type of my env.action(). Will possibly preprocess this and use it as input for my model
12) Thinking of DQN + PPO approach to build and train a strong model
13) Studying the 'state' N-dimensional vector. This will prove out to be helpful
14) Starting to think from first principles and am this time thinking of CNN + PPO.
15) Since I have no labelled data, I am trying to formulate an approach to build and train an agent
16) Studied stable_baseline3 library to implement PPO reinforcement learning algorithm
17) Implementing the PPO(cnn_policy) and setting things up
18) Facing multiple issues in car_dynamics.py. Fixing those internal errors manually
19) So far worked on fixing the data type issues and the incompatability with box-2d
20) Installing box2d-py, for which my linux machine required 'swig'
21) A reminder that for this project, one also needs to directly use my 'venv' because there are certain internal changes which I have made in the modules of the OpenAI's gym library and NumPy's library to handle certain compatability and type issues
22) Converted np.float32 and np.float64 types to basic Python float in point coordinates before calling pygame.draw.polygon() to ensure compatibility and avoid potential issues with pygame rendering. Improved code stability by handling different numeric types in the points list.
23) It worked!!! I made multiple changes to car_dynamics.py, car_racing.py, polygon functions and point tuples
24) Finally, the model is training and I can even view the simulation
25) There are actually a new set of issues:
 a) total_timesteps (the number of actions taken by the agent) is not making any difference
 b) The simulation starts with the car moving and basically the model running it for around 10 seconds, after which the simulation automatically resets. This happens 3 times and at the end of the third reset and re-run of the simulation, I get these:
"
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -54.6    |
| time/              |          |
|    fps             | 42       |
|    iterations      | 1        |
|    time_elapsed    | 48       |
|    total_timesteps | 2048     |
---------------------------------
"

Right after I get these my pygame/simulation gui freezes and I get this pop-up that pygame is not responding

Trying to do something about these
26) Fixed issue a) by studying the documentation and finding out a new parameter n_steps in PPO's Cnnpolicy.
The reason why total_timesteps was not making any difference was because technically n_steps of PPO's Cnnpolicy takes  has more priority in deciding
the number of steps the agent will take. Hence, we need to set both of them actually.

27) Fixed the second issue by further simplification and further cross examination of the working of each 'episode' of an agent
28)
'
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -57.5    |
| time/              |          |
|    fps             | 47       |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 1000     |
---------------------------------
2025-02-07 09:44:22 - âœ… Training Complete. Saving model...
2025-02-07 09:44:22 - ðŸ’¾ Model saved as 'ppo_CarRacing-v2_cnn'.
'
My next target is to increase the number of iterations and slowly keep improving over the system until the Episode Reward Mean doesn't become in positive and then greater than 100 or so
29) It still freezes. Trying to change render_mode in gym.make() method
30) - Replacing `gym` with `gymnasium`, removing `render_mode='human'`, increasing `batch_size` to 64 and `n_steps` to 2048, reducing logging verbosity, optimizing logs to prevent PyCharm freeze, and ensuring model checkpoints are saving correctlyâ€”improving stability, performance, and training efficiency while preventing crashes.
31) It was due to me trying to visualize the training parallely using PyCharm. Hence, I closed that.
32) I want to make sure that every model training is unique, and its name also includes the ep_reward_mean value. Tell me a trick to make sure every single training that I do, creates a completely new model which is saved uniquely (ensuring no training is lost) and I can see its ep_reward_mean value
33) I trained the model with a batch size of 128, training_steps of 10000 for 100 iterations. It did not save but its logs were. I carried out some
statistical analysis on the results of the logs to test the model's performance in each iteration. The results are in 'Model training statistical analysis.ipynb'
34) I just tested the 798 ep_reward_mean AI model and it runs well. Here are some characteristics of the model that I have seen so far:
a) It likes to stay at the left side of the road
b) When it starts going really off-track, it knows how to use the brakes and the steering to come right back on track
c) It tends to prioritize speed over accuracy in many cases
d) If I were to give it a name, it will be: Chaotic Teenager
Next target: do some more research and study its behavior, all of its states, how is the internal CnnPolicy with PPO working. I will be doing this for 1-2 days. Then, I will br proceeding with the experimentation of creating new AI models for the same (moving on from Chaotic Teenager)